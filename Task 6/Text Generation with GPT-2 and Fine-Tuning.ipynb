{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Text Generation with GPT-2 and Fine-Tuning <B>#"
      ],
      "metadata": {
        "id": "BNp-wU7jtmSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu sentence-transformers transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCVgy-ASY24J",
        "outputId": "d1ba7ce8-60fe-4681-8fc5-7669446da0e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Adjust tokenizer for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Custom Dataset for Text Generation\n",
        "class TextGenerationDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        # Tokenize the input text and convert it into input_ids for GPT-2\n",
        "        self.examples = tokenizer(text, return_tensors=\"pt\", max_length=block_size, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Paths to dataset files\n",
        "train_file_path = \"/content/train.txt\"  # Update with your training data file path\n",
        "test_file_path = \"/content/test.txt\"    # Update with your test data file path\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = TextGenerationDataset(train_file_path, tokenizer)\n",
        "test_dataset = TextGenerationDataset(test_file_path, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 3\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch, labels=batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n",
        "print(\"Fine-tuned model saved.\")\n",
        "\n",
        "# Evaluation Loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    eval_loss = 0\n",
        "    for batch in test_dataloader:\n",
        "        batch = batch.to(device)\n",
        "        outputs = model(input_ids=batch, labels=batch)\n",
        "        loss = outputs.loss\n",
        "        eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(test_dataloader)\n",
        "    print(f\"Evaluation Loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "# Function to generate text with fine-tuned model\n",
        "def generate_text(prompt, model, tokenizer, max_length=350, temperature=0.7, top_k=50, top_p=0.95):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,  # Enable sampling to allow creativity in the output\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Alternative example: Text generation with user input\n",
        "print(\"\\nEnter a prompt for text generation:\")\n",
        "user_prompt = input().strip()\n",
        "\n",
        "generated_text_user = generate_text(user_prompt, model, tokenizer)\n",
        "print(f\"\\nGenerated Text for your prompt:\\n{generated_text_user}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veBlB4RoY8YG",
        "outputId": "8d6cae85-caac-45d7-f772-f9be1eb3c273"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Loss: 2.5187\n",
            "Epoch [2/3], Loss: 1.9273\n",
            "Epoch [3/3], Loss: 1.2865\n",
            "Fine-tuned model saved.\n",
            "Evaluation Loss: 2.0357\n",
            "\n",
            "Enter a prompt for text generation:\n",
            "there is a story\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text for your prompt:\n",
            "there is a story that the President told a reporter about a woman he met while he was in the White House. The story says that when he went to the White House, he asked her, 'What are you doing?' and she said, 'I'm in a room with the President.' He said, 'I don't know what I'm doing.' She said, 'You're not going to tell me anything.' He said, 'No, I'm not going to tell you anything. I'm going to make up stories.'\"\n",
            "\n",
            "It's possible that the two stories are connected, as some have suggested. But a new study by the National Center for Missing and Exploited Children and the National Center for Missing and Exploited Children at New York University's Langone Center for Child and Adolescent Mental Health finds that \"no substantial connection exists between the two stories.\"\n",
            "\n",
            "\"It's a very interesting study,\" said William C. B. Kugel, a social psychologist at New York University and the lead author of the study. \"We can see that people who don't know the story are less likely to believe it. So we have to go back to that question of whether the two stories are connected.\"\n",
            "\n",
            "Kugel and his colleagues have also done a larger study on the relationship between social media and the spread of criminal activity.\n",
            "\n",
            "\"I think it's important to understand where we're coming from,\" Kugel said. \"There's a lot of research that suggests that people who use social media, whether they are a journalist or an activist, are more likely to commit criminal offenses. But it also helps us to understand the dynamics of the online community, and that there's a lot\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}